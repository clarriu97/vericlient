{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vericlient","text":"<p>Vericlient is a Python library designed to facilitate interaction with the Veridas API.</p>"},{"location":"#installation","title":"Installation","text":"<p>Installation is as simple as:</p> <pre><code>pip install vericlient\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Visit the API Documentation to learn how to use the library.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>--8&lt;-- \"../CHANGELOG.md\"</p>"},{"location":"docs/","title":"Docs","text":"<p>This is the documentation for the project.</p>"},{"location":"api_docs/vericlient/","title":"Modules","text":"<p>The <code>vericlient</code> module provides a way to interact with the Veridas APIs.</p> <p>From it, you can import the different clients to be used for each service.</p>"},{"location":"api_docs/vericlient/#vericlient","title":"<code>vericlient</code>","text":"<p>vericlient module.</p>"},{"location":"api_docs/vericlient/#vericlient.DaspeakClient","title":"<code>DaspeakClient</code>","text":"<p>               Bases: <code>Client</code></p> <p>Class to interact with the Daspeak API.</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>class DaspeakClient(Client):\n    \"\"\"Class to interact with the Daspeak API.\"\"\"\n\n    def __init__(\n            self,\n            api: str = APIs.DASPEAK.value,\n            apikey: str | None = None,\n            target: str | None = None,\n            timeout: int | None = None,\n            environment: str | None = None,\n            location: str | None = None,\n            url: str | None = None,\n            headers: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Create the DaspeakClient class.\n\n        Args:\n            api: The API to use\n            apikey: The API key to use\n            target: The target to use\n            timeout: The timeout to use in the requests\n            environment: The environment to use\n            location: The location to use\n            url: The URL to use in case of a custom target\n            headers: The headers to be used in the requests\n\n        \"\"\"\n        super().__init__(\n            api=api,\n            apikey=apikey,\n            target=target,\n            timeout=timeout,\n            environment=environment,\n            location=location,\n            url=url,\n            headers=headers,\n        )\n        self._exceptions = [\n            \"InputException\",\n            \"SignalNoiseRatioException\",\n            \"VoiceDurationIsNotEnoughException\",\n            \"InvalidChannelException\",\n            \"InsufficientQuality\",\n            \"CalibrationNotAvailable\",\n            \"ServerError\",\n        ]\n\n    def alive(self) -&gt; bool:\n        \"\"\"Check if the service is alive.\n\n        Returns\n            bool: True if the service is alive, False otherwise\n\n        \"\"\"\n        response = self._get(endpoint=DaspeakEndpoints.ALIVE.value)\n        accepted_status_code = 200\n        return response.status_code == accepted_status_code\n\n    def _handle_error_response(self, response: Response) -&gt; None:   # noqa: C901\n        \"\"\"Handle error responses from the API.\"\"\"\n        response_json = response.json()\n        if response_json[\"exception\"] not in self._exceptions:\n            self._raise_server_error(response)\n        if response_json[\"exception\"] == \"InputException\":\n            if \"more channels than\" in response_json[\"error\"]:\n                raise TooManyAudioChannelsError\n            if \"unsupported codec\" in response_json[\"error\"]:\n                raise UnsupportedAudioCodecError\n            if \"sample rate\" in response_json[\"error\"]:\n                raise UnsupportedSampleRateError\n            if \"duration is longer\" in response_json[\"error\"]:\n                raise AudioDurationTooLongError\n            raise ValueError(response_json[\"error\"])\n        if response_json[\"exception\"] == \"SignalNoiseRatioException\":\n            raise SignalNoiseRatioError\n        if response_json[\"exception\"] == \"VoiceDurationIsNotEnoughException\":\n            error = response_json[\"error\"]\n            net_speech_detected = float(error.split(\" \")[-3].replace(\"s\", \"\"))\n            raise NetSpeechDurationIsNotEnoughError(net_speech_detected)\n        if response_json[\"exception\"] == \"InvalidChannelException\":\n            raise InvalidSpecifiedChannelError\n        if response_json[\"exception\"] == \"InsufficientQuality\":\n            raise InsufficientQualityError\n        if response_json[\"exception\"] == \"CalibrationNotAvailable\":\n            raise CalibrationNotAvailableError\n        raise ValueError(response_json[\"error\"])\n\n    def get_models(self) -&gt; ModelsOutput:\n        \"\"\"Get the models available biometrics models in the service.\n\n        Returns\n            ModelsOutput: The response from the service\n\n        \"\"\"\n        response = self._get(endpoint=DaspeakEndpoints.MODELS.value)\n        return ModelsOutput(status_code=response.status_code, **response.json())\n\n    def generate_credential(self, data_model: ModelsHashCredentialWavInput) -&gt; ModelsHashCredentialWavOutput:\n        \"\"\"Generate a credential from a WAV file.\n\n        **Warning**: if the audio provided is a `BytesIO` object, make sure to close it after using this method.\n\n        Args:\n            data_model: The data required to generate the credential\n\n        Returns:\n            ModelsHashCredentialWavOutput: The response from the service\n\n        \"\"\"\n        endpoint = DaspeakEndpoints.MODELS_HASH_CREDENTIAL_WAV.value.replace(\"&lt;hash&gt;\", data_model.hash)\n        audio = self._get_virtual_audio_file(data_model.audio)\n        files = {\n            \"audio\": (\"audio\", audio.read(), \"audio/wav\"),\n        }\n        data = {\n            \"channel\": data_model.channel,\n            \"calibration\": data_model.calibration,\n        }\n        response = self._post(endpoint=endpoint, data=data, files=files)\n        return ModelsHashCredentialWavOutput(status_code=response.status_code, **response.json())\n\n    def _get_virtual_audio_file(self, audio_input: object) -&gt; BytesIO:\n        if isinstance(audio_input, str):\n            try:\n                audio = open(audio_input, \"rb\")     # noqa: SIM115\n            except FileNotFoundError as e:\n                error = f\"File {audio_input} not found\"\n                raise FileNotFoundError(error) from e\n        elif isinstance(audio_input, BytesIO):\n            audio = audio_input\n        else:\n            error = \"audio must be a string or a BytesIO object\"\n            raise TypeError(error)\n        return audio\n</code></pre>"},{"location":"api_docs/vericlient/#vericlient.DaspeakClient.__init__","title":"<code>__init__(api=APIs.DASPEAK.value, apikey=None, target=None, timeout=None, environment=None, location=None, url=None, headers=None)</code>","text":"<p>Create the DaspeakClient class.</p> <p>Parameters:</p> Name Type Description Default <code>api</code> <code>str</code> <p>The API to use</p> <code>DASPEAK.value</code> <code>apikey</code> <code>str | None</code> <p>The API key to use</p> <code>None</code> <code>target</code> <code>str | None</code> <p>The target to use</p> <code>None</code> <code>timeout</code> <code>int | None</code> <p>The timeout to use in the requests</p> <code>None</code> <code>environment</code> <code>str | None</code> <p>The environment to use</p> <code>None</code> <code>location</code> <code>str | None</code> <p>The location to use</p> <code>None</code> <code>url</code> <code>str | None</code> <p>The URL to use in case of a custom target</p> <code>None</code> <code>headers</code> <code>dict | None</code> <p>The headers to be used in the requests</p> <code>None</code> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def __init__(\n        self,\n        api: str = APIs.DASPEAK.value,\n        apikey: str | None = None,\n        target: str | None = None,\n        timeout: int | None = None,\n        environment: str | None = None,\n        location: str | None = None,\n        url: str | None = None,\n        headers: dict | None = None,\n) -&gt; None:\n    \"\"\"Create the DaspeakClient class.\n\n    Args:\n        api: The API to use\n        apikey: The API key to use\n        target: The target to use\n        timeout: The timeout to use in the requests\n        environment: The environment to use\n        location: The location to use\n        url: The URL to use in case of a custom target\n        headers: The headers to be used in the requests\n\n    \"\"\"\n    super().__init__(\n        api=api,\n        apikey=apikey,\n        target=target,\n        timeout=timeout,\n        environment=environment,\n        location=location,\n        url=url,\n        headers=headers,\n    )\n    self._exceptions = [\n        \"InputException\",\n        \"SignalNoiseRatioException\",\n        \"VoiceDurationIsNotEnoughException\",\n        \"InvalidChannelException\",\n        \"InsufficientQuality\",\n        \"CalibrationNotAvailable\",\n        \"ServerError\",\n    ]\n</code></pre>"},{"location":"api_docs/vericlient/#vericlient.DaspeakClient.alive","title":"<code>alive()</code>","text":"<p>Check if the service is alive.</p> <p>Returns     bool: True if the service is alive, False otherwise</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def alive(self) -&gt; bool:\n    \"\"\"Check if the service is alive.\n\n    Returns\n        bool: True if the service is alive, False otherwise\n\n    \"\"\"\n    response = self._get(endpoint=DaspeakEndpoints.ALIVE.value)\n    accepted_status_code = 200\n    return response.status_code == accepted_status_code\n</code></pre>"},{"location":"api_docs/vericlient/#vericlient.DaspeakClient.generate_credential","title":"<code>generate_credential(data_model)</code>","text":"<p>Generate a credential from a WAV file.</p> <p>Warning: if the audio provided is a <code>BytesIO</code> object, make sure to close it after using this method.</p> <p>Parameters:</p> Name Type Description Default <code>data_model</code> <code>ModelsHashCredentialWavInput</code> <p>The data required to generate the credential</p> required <p>Returns:</p> Name Type Description <code>ModelsHashCredentialWavOutput</code> <code>ModelsHashCredentialWavOutput</code> <p>The response from the service</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def generate_credential(self, data_model: ModelsHashCredentialWavInput) -&gt; ModelsHashCredentialWavOutput:\n    \"\"\"Generate a credential from a WAV file.\n\n    **Warning**: if the audio provided is a `BytesIO` object, make sure to close it after using this method.\n\n    Args:\n        data_model: The data required to generate the credential\n\n    Returns:\n        ModelsHashCredentialWavOutput: The response from the service\n\n    \"\"\"\n    endpoint = DaspeakEndpoints.MODELS_HASH_CREDENTIAL_WAV.value.replace(\"&lt;hash&gt;\", data_model.hash)\n    audio = self._get_virtual_audio_file(data_model.audio)\n    files = {\n        \"audio\": (\"audio\", audio.read(), \"audio/wav\"),\n    }\n    data = {\n        \"channel\": data_model.channel,\n        \"calibration\": data_model.calibration,\n    }\n    response = self._post(endpoint=endpoint, data=data, files=files)\n    return ModelsHashCredentialWavOutput(status_code=response.status_code, **response.json())\n</code></pre>"},{"location":"api_docs/vericlient/#vericlient.DaspeakClient.get_models","title":"<code>get_models()</code>","text":"<p>Get the models available biometrics models in the service.</p> <p>Returns     ModelsOutput: The response from the service</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def get_models(self) -&gt; ModelsOutput:\n    \"\"\"Get the models available biometrics models in the service.\n\n    Returns\n        ModelsOutput: The response from the service\n\n    \"\"\"\n    response = self._get(endpoint=DaspeakEndpoints.MODELS.value)\n    return ModelsOutput(status_code=response.status_code, **response.json())\n</code></pre>"},{"location":"api_docs/vericlient/#vericlient.Environments","title":"<code>Environments</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Represents the environments that the client can make requests to, if the target is the cloud.</p> Source code in <code>src/vericlient/environments.py</code> <pre><code>class Environments(Enum):\n    \"\"\"Represents the environments that the client can make requests to, if the target is the cloud.\"\"\"\n\n    SANDBOX = \"sandbox\"\n    PRODUCTION = \"production\"\n</code></pre>"},{"location":"api_docs/vericlient/#vericlient.Locations","title":"<code>Locations</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Represents the locations that the client can make requests to, if the target is the cloud.</p> Source code in <code>src/vericlient/environments.py</code> <pre><code>class Locations(Enum):\n    \"\"\"Represents the locations that the client can make requests to, if the target is the cloud.\"\"\"\n\n    EU = \"eu\"\n    US = \"us\"\n</code></pre>"},{"location":"api_docs/vericlient/#vericlient.Target","title":"<code>Target</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Represents the target the client is going to make requests to.</p> Source code in <code>src/vericlient/environments.py</code> <pre><code>class Target(Enum):\n    \"\"\"Represents the target the client is going to make requests to.\"\"\"\n\n    CLOUD = \"cloud\"\n    CUSTOM = \"custom\"\n</code></pre>"},{"location":"api_docs/daspeak/client/","title":"Daspeak Client","text":"<p>This is an example of how to use the Daspeak client:</p> <pre><code>from io import BytesIO\n\nfrom vericlient import DaspeakClient\nfrom vericlient.daspeak.models import ModelsHashCredentialWavInput\n\n\nclient = DaspeakClient(apikey=\"your_api_key\")\n\n# check if the server is alive\nprint(f\"Alive: {client.alive()}\")\n\n# get the available biometrics models\nprint(f\"Biometrics models: {client.get_models().models}\")\n\n# generate a credential from an audio file using the last model\nmodel_input = ModelsHashCredentialWavInput(\n    audio=\"/home/audio.wav\",\n    hash=client.get_models().models[-1],\n)\nmodel_output = client.generate_credential(model_input)\nprint(f\"Credential generated with an audio file: {model_output.credential}\")\n\n# generate a credential from a BytesIO object using the last model\nwith open(\"/home/audio.wav\", \"rb\") as f:\n    model_input = ModelsHashCredentialWavInput(\n        audio=BytesIO(f.read()),\n        hash=client.get_models().models[-1]\n    )\nmodel_output = client.generate_credential(model_input)\nprint(f\"Credential generated with virtual file: {model_output.credential}\")\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient","title":"<code>vericlient.daspeak.client.DaspeakClient</code>","text":"<p>               Bases: <code>Client</code></p> <p>Class to interact with the Daspeak API.</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>class DaspeakClient(Client):\n    \"\"\"Class to interact with the Daspeak API.\"\"\"\n\n    def __init__(\n            self,\n            api: str = APIs.DASPEAK.value,\n            apikey: str | None = None,\n            target: str | None = None,\n            timeout: int | None = None,\n            environment: str | None = None,\n            location: str | None = None,\n            url: str | None = None,\n            headers: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Create the DaspeakClient class.\n\n        Args:\n            api: The API to use\n            apikey: The API key to use\n            target: The target to use\n            timeout: The timeout to use in the requests\n            environment: The environment to use\n            location: The location to use\n            url: The URL to use in case of a custom target\n            headers: The headers to be used in the requests\n\n        \"\"\"\n        super().__init__(\n            api=api,\n            apikey=apikey,\n            target=target,\n            timeout=timeout,\n            environment=environment,\n            location=location,\n            url=url,\n            headers=headers,\n        )\n        self._exceptions = [\n            \"InputException\",\n            \"SignalNoiseRatioException\",\n            \"VoiceDurationIsNotEnoughException\",\n            \"InvalidChannelException\",\n            \"InsufficientQuality\",\n            \"CalibrationNotAvailable\",\n            \"ServerError\",\n        ]\n\n    def alive(self) -&gt; bool:\n        \"\"\"Check if the service is alive.\n\n        Returns\n            bool: True if the service is alive, False otherwise\n\n        \"\"\"\n        response = self._get(endpoint=DaspeakEndpoints.ALIVE.value)\n        accepted_status_code = 200\n        return response.status_code == accepted_status_code\n\n    def _handle_error_response(self, response: Response) -&gt; None:   # noqa: C901\n        \"\"\"Handle error responses from the API.\"\"\"\n        response_json = response.json()\n        if response_json[\"exception\"] not in self._exceptions:\n            self._raise_server_error(response)\n        if response_json[\"exception\"] == \"InputException\":\n            if \"more channels than\" in response_json[\"error\"]:\n                raise TooManyAudioChannelsError\n            if \"unsupported codec\" in response_json[\"error\"]:\n                raise UnsupportedAudioCodecError\n            if \"sample rate\" in response_json[\"error\"]:\n                raise UnsupportedSampleRateError\n            if \"duration is longer\" in response_json[\"error\"]:\n                raise AudioDurationTooLongError\n            raise ValueError(response_json[\"error\"])\n        if response_json[\"exception\"] == \"SignalNoiseRatioException\":\n            raise SignalNoiseRatioError\n        if response_json[\"exception\"] == \"VoiceDurationIsNotEnoughException\":\n            error = response_json[\"error\"]\n            net_speech_detected = float(error.split(\" \")[-3].replace(\"s\", \"\"))\n            raise NetSpeechDurationIsNotEnoughError(net_speech_detected)\n        if response_json[\"exception\"] == \"InvalidChannelException\":\n            raise InvalidSpecifiedChannelError\n        if response_json[\"exception\"] == \"InsufficientQuality\":\n            raise InsufficientQualityError\n        if response_json[\"exception\"] == \"CalibrationNotAvailable\":\n            raise CalibrationNotAvailableError\n        raise ValueError(response_json[\"error\"])\n\n    def get_models(self) -&gt; ModelsOutput:\n        \"\"\"Get the models available biometrics models in the service.\n\n        Returns\n            ModelsOutput: The response from the service\n\n        \"\"\"\n        response = self._get(endpoint=DaspeakEndpoints.MODELS.value)\n        return ModelsOutput(status_code=response.status_code, **response.json())\n\n    def generate_credential(self, data_model: ModelsHashCredentialWavInput) -&gt; ModelsHashCredentialWavOutput:\n        \"\"\"Generate a credential from a WAV file.\n\n        **Warning**: if the audio provided is a `BytesIO` object, make sure to close it after using this method.\n\n        Args:\n            data_model: The data required to generate the credential\n\n        Returns:\n            ModelsHashCredentialWavOutput: The response from the service\n\n        \"\"\"\n        endpoint = DaspeakEndpoints.MODELS_HASH_CREDENTIAL_WAV.value.replace(\"&lt;hash&gt;\", data_model.hash)\n        audio = self._get_virtual_audio_file(data_model.audio)\n        files = {\n            \"audio\": (\"audio\", audio.read(), \"audio/wav\"),\n        }\n        data = {\n            \"channel\": data_model.channel,\n            \"calibration\": data_model.calibration,\n        }\n        response = self._post(endpoint=endpoint, data=data, files=files)\n        return ModelsHashCredentialWavOutput(status_code=response.status_code, **response.json())\n\n    def _get_virtual_audio_file(self, audio_input: object) -&gt; BytesIO:\n        if isinstance(audio_input, str):\n            try:\n                audio = open(audio_input, \"rb\")     # noqa: SIM115\n            except FileNotFoundError as e:\n                error = f\"File {audio_input} not found\"\n                raise FileNotFoundError(error) from e\n        elif isinstance(audio_input, BytesIO):\n            audio = audio_input\n        else:\n            error = \"audio must be a string or a BytesIO object\"\n            raise TypeError(error)\n        return audio\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient.__init__","title":"<code>__init__(api=APIs.DASPEAK.value, apikey=None, target=None, timeout=None, environment=None, location=None, url=None, headers=None)</code>","text":"<p>Create the DaspeakClient class.</p> <p>Parameters:</p> Name Type Description Default <code>api</code> <code>str</code> <p>The API to use</p> <code>DASPEAK.value</code> <code>apikey</code> <code>str | None</code> <p>The API key to use</p> <code>None</code> <code>target</code> <code>str | None</code> <p>The target to use</p> <code>None</code> <code>timeout</code> <code>int | None</code> <p>The timeout to use in the requests</p> <code>None</code> <code>environment</code> <code>str | None</code> <p>The environment to use</p> <code>None</code> <code>location</code> <code>str | None</code> <p>The location to use</p> <code>None</code> <code>url</code> <code>str | None</code> <p>The URL to use in case of a custom target</p> <code>None</code> <code>headers</code> <code>dict | None</code> <p>The headers to be used in the requests</p> <code>None</code> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def __init__(\n        self,\n        api: str = APIs.DASPEAK.value,\n        apikey: str | None = None,\n        target: str | None = None,\n        timeout: int | None = None,\n        environment: str | None = None,\n        location: str | None = None,\n        url: str | None = None,\n        headers: dict | None = None,\n) -&gt; None:\n    \"\"\"Create the DaspeakClient class.\n\n    Args:\n        api: The API to use\n        apikey: The API key to use\n        target: The target to use\n        timeout: The timeout to use in the requests\n        environment: The environment to use\n        location: The location to use\n        url: The URL to use in case of a custom target\n        headers: The headers to be used in the requests\n\n    \"\"\"\n    super().__init__(\n        api=api,\n        apikey=apikey,\n        target=target,\n        timeout=timeout,\n        environment=environment,\n        location=location,\n        url=url,\n        headers=headers,\n    )\n    self._exceptions = [\n        \"InputException\",\n        \"SignalNoiseRatioException\",\n        \"VoiceDurationIsNotEnoughException\",\n        \"InvalidChannelException\",\n        \"InsufficientQuality\",\n        \"CalibrationNotAvailable\",\n        \"ServerError\",\n    ]\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient.alive","title":"<code>alive()</code>","text":"<p>Check if the service is alive.</p> <p>Returns     bool: True if the service is alive, False otherwise</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def alive(self) -&gt; bool:\n    \"\"\"Check if the service is alive.\n\n    Returns\n        bool: True if the service is alive, False otherwise\n\n    \"\"\"\n    response = self._get(endpoint=DaspeakEndpoints.ALIVE.value)\n    accepted_status_code = 200\n    return response.status_code == accepted_status_code\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient.generate_credential","title":"<code>generate_credential(data_model)</code>","text":"<p>Generate a credential from a WAV file.</p> <p>Warning: if the audio provided is a <code>BytesIO</code> object, make sure to close it after using this method.</p> <p>Parameters:</p> Name Type Description Default <code>data_model</code> <code>ModelsHashCredentialWavInput</code> <p>The data required to generate the credential</p> required <p>Returns:</p> Name Type Description <code>ModelsHashCredentialWavOutput</code> <code>ModelsHashCredentialWavOutput</code> <p>The response from the service</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def generate_credential(self, data_model: ModelsHashCredentialWavInput) -&gt; ModelsHashCredentialWavOutput:\n    \"\"\"Generate a credential from a WAV file.\n\n    **Warning**: if the audio provided is a `BytesIO` object, make sure to close it after using this method.\n\n    Args:\n        data_model: The data required to generate the credential\n\n    Returns:\n        ModelsHashCredentialWavOutput: The response from the service\n\n    \"\"\"\n    endpoint = DaspeakEndpoints.MODELS_HASH_CREDENTIAL_WAV.value.replace(\"&lt;hash&gt;\", data_model.hash)\n    audio = self._get_virtual_audio_file(data_model.audio)\n    files = {\n        \"audio\": (\"audio\", audio.read(), \"audio/wav\"),\n    }\n    data = {\n        \"channel\": data_model.channel,\n        \"calibration\": data_model.calibration,\n    }\n    response = self._post(endpoint=endpoint, data=data, files=files)\n    return ModelsHashCredentialWavOutput(status_code=response.status_code, **response.json())\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient.get_models","title":"<code>get_models()</code>","text":"<p>Get the models available biometrics models in the service.</p> <p>Returns     ModelsOutput: The response from the service</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def get_models(self) -&gt; ModelsOutput:\n    \"\"\"Get the models available biometrics models in the service.\n\n    Returns\n        ModelsOutput: The response from the service\n\n    \"\"\"\n    response = self._get(endpoint=DaspeakEndpoints.MODELS.value)\n    return ModelsOutput(status_code=response.status_code, **response.json())\n</code></pre>"},{"location":"api_docs/daspeak/models/","title":"Daspeak Models","text":""},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models","title":"<code>vericlient.daspeak.models</code>","text":"<p>Module to define the models for the Daspeak API.</p>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.DaspeakResponse","title":"<code>DaspeakResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for the Daspeak API responses.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>str</code> <p>the version of the API</p> <code>status_code</code> <code>int</code> <p>the status code of the response</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class DaspeakResponse(BaseModel):\n    \"\"\"Base class for the Daspeak API responses.\n\n    Attributes:\n        version: the version of the API\n        status_code: the status code of the response\n\n    \"\"\"\n\n    version: str\n    status_code: int\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.ModelMetadata","title":"<code>ModelMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata of the model used to generate the credential.</p> <p>Attributes:</p> Name Type Description <code>hash</code> <code>str</code> <p>the hash of the model</p> <code>mode</code> <code>str</code> <p>the mode of the model</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class ModelMetadata(BaseModel):\n    \"\"\"Metadata of the model used to generate the credential.\n\n    Attributes:\n        hash: the hash of the model\n        mode: the mode of the model\n\n    \"\"\"\n\n    hash: str\n    mode: str\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.ModelsHashCredentialWavInput","title":"<code>ModelsHashCredentialWavInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input class for the generate credential endpoint.</p> <p>Attributes:</p> Name Type Description <code>audio</code> <code>str | BytesIO</code> <p>the audio to generate the credential with</p> <code>hash</code> <code>str</code> <p>the hash of the biometrics model to use</p> <code>channel</code> <code>int</code> <p>the <code>nchannel</code> of the audio if it is stereo</p> <code>calibration</code> <code>str</code> <p>the calibration to use</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class ModelsHashCredentialWavInput(BaseModel):\n    \"\"\"Input class for the generate credential endpoint.\n\n    Attributes:\n        audio: the audio to generate the credential with\n        hash: the hash of the biometrics model to use\n        channel: the `nchannel` of the audio if it is stereo\n        calibration: the calibration to use\n\n    \"\"\"\n\n    audio: str | BytesIO\n    hash: str\n    channel: int = 1\n    calibration: str = \"telephone-channel\"\n\n    @field_validator(\"audio\")\n    def must_be_str_or_bytesio(cls, value: object):\n        if not isinstance(value, (str, BytesIO)):\n            error = \"audio must be a string or a BytesIO object\"\n            raise TypeError(error)\n        return value\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.ModelsHashCredentialWavOutput","title":"<code>ModelsHashCredentialWavOutput</code>","text":"<p>               Bases: <code>DaspeakResponse</code></p> <p>Output class for the generate credential endpoint.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>ModelMetadata</code> <p>the model used to generate the credential</p> <code>credential</code> <code>str</code> <p>the generated credential</p> <code>authenticity</code> <code>float</code> <p>the authenticity of the audio sample used</p> <code>input_audio_duration</code> <code>float</code> <p>the duration of the input audio</p> <code>net_speech_duration</code> <code>float</code> <p>the duration of the speech in the audio</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class ModelsHashCredentialWavOutput(DaspeakResponse):\n    \"\"\"Output class for the generate credential endpoint.\n\n    Attributes:\n        model: the model used to generate the credential\n        credential: the generated credential\n        authenticity: the authenticity of the audio sample used\n        input_audio_duration: the duration of the input audio\n        net_speech_duration: the duration of the speech in the audio\n\n    \"\"\"\n\n    model: ModelMetadata\n    credential: str\n    authenticity: float\n    input_audio_duration: float\n    net_speech_duration: float\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.ModelsOutput","title":"<code>ModelsOutput</code>","text":"<p>               Bases: <code>DaspeakResponse</code></p> <p>Output class for the get models endpoint.</p> <p>Attributes:</p> Name Type Description <code>models</code> <code>list</code> <p>the available models in the service</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class ModelsOutput(DaspeakResponse):\n    \"\"\"Output class for the get models endpoint.\n\n    Attributes:\n        models: the available models in the service\n\n    \"\"\"\n\n    models: list\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.SimilarityCredential2CredentialInput","title":"<code>SimilarityCredential2CredentialInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input class for the similarity credential to credential endpoint.</p> <p>Attributes:</p> Name Type Description <code>credential_reference</code> <code>str</code> <p>the reference credential</p> <code>credential_to_evaluate</code> <code>str</code> <p>the credential to evaluate</p> <code>calibration</code> <code>str</code> <p>the calibration to use</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class SimilarityCredential2CredentialInput(BaseModel):\n    \"\"\"Input class for the similarity credential to credential endpoint.\n\n    Attributes:\n        credential_reference: the reference credential\n        credential_to_evaluate: the credential to evaluate\n        calibration: the calibration to use\n\n    \"\"\"\n\n    credential_reference: str\n    credential_to_evaluate: str\n    caliration: str = \"telephone-channel\"\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.SimilarityCredential2CredentialOutput","title":"<code>SimilarityCredential2CredentialOutput</code>","text":"<p>               Bases: <code>DaspeakResponse</code></p> <p>Output class for the similarity credential to credential endpoint.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>the similarity score between the two credentials</p> <code>calibration</code> <code>str</code> <p>the calibration used</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class SimilarityCredential2CredentialOutput(DaspeakResponse):\n    \"\"\"Output class for the similarity credential to credential endpoint.\n\n    Attributes:\n        score: the similarity score between the two credentials\n        calibration: the calibration used\n\n    \"\"\"\n\n    calibration: str\n    score: float\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.SimilarityCredential2WavInput","title":"<code>SimilarityCredential2WavInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input class for the similarity credential to wav endpoint.</p> <p>Attributes:</p> Name Type Description <code>credential_reference</code> <code>str</code> <p>the reference credential</p> <code>audio_to_evaluate</code> <code>str | BytesIO</code> <p>the audio to evaluate</p> <code>channel</code> <code>int</code> <p>the <code>nchannel</code> of the audio if it is stereo</p> <code>calibration</code> <code>str</code> <p>the calibration to use</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class SimilarityCredential2WavInput(BaseModel):\n    \"\"\"Input class for the similarity credential to wav endpoint.\n\n    Attributes:\n        credential_reference: the reference credential\n        audio_to_evaluate: the audio to evaluate\n        channel: the `nchannel` of the audio if it is stereo\n        calibration: the calibration to use\n\n    \"\"\"\n\n    credential_reference: str\n    audio_to_evaluate: str | BytesIO\n    channel: int = 1\n    calibration: str = \"telephone-channel\"\n\n    @field_validator(\"audio_to_evaluate\")\n    def must_be_str_or_bytesio(cls, value: object):\n        if not isinstance(value, (str, BytesIO)):\n            error = \"audio must be a string or a BytesIO object\"\n            raise TypeError(error)\n        return value\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.SimilarityCredential2WavOutput","title":"<code>SimilarityCredential2WavOutput</code>","text":"<p>               Bases: <code>DaspeakResponse</code></p> <p>Output class for the similarity credential to wav endpoint.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>the similarity score between the credential and the audio</p> <code>model</code> <code>ModelMetadata</code> <p>the model used to generate the credential</p> <code>calibration</code> <code>str</code> <p>the calibration used</p> <code>authenticity_to_evaluate</code> <code>float</code> <p>the authenticity of the audio sample used</p> <code>input_audio_duration</code> <code>float</code> <p>the duration of the input audio</p> <code>net_speech_duration</code> <code>float</code> <p>the duration of the speech in the audio</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class SimilarityCredential2WavOutput(DaspeakResponse):\n    \"\"\"Output class for the similarity credential to wav endpoint.\n\n    Attributes:\n        score: the similarity score between the credential and the audio\n        model: the model used to generate the credential\n        calibration: the calibration used\n        authenticity_to_evaluate: the authenticity of the audio sample used\n        input_audio_duration: the duration of the input audio\n        net_speech_duration: the duration of the speech in the audio\n\n    \"\"\"\n\n    score: float\n    model: ModelMetadata\n    calibration: str\n    authenticity_to_evaluate: float\n    input_audio_duration: float\n    net_speech_duration: float\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.SimilarityWav2WavInput","title":"<code>SimilarityWav2WavInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input class for the similarity wav to wav endpoint.</p> <p>Attributes:</p> Name Type Description <code>audio_reference</code> <code>str | BytesIO</code> <p>the reference audio</p> <code>audio_to_evaluate</code> <code>str | BytesIO</code> <p>the audio to evaluate</p> <code>channel_reference</code> <code>int</code> <p>the <code>nchannel</code> of the reference audio if it is stereo</p> <code>channel_to_evaluate</code> <code>int</code> <p>the <code>nchannel</code> of the audio to evaluate if it is stereo</p> <code>calibration</code> <code>str</code> <p>the calibration to use</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class SimilarityWav2WavInput(BaseModel):\n    \"\"\"Input class for the similarity wav to wav endpoint.\n\n    Attributes:\n        audio_reference: the reference audio\n        audio_to_evaluate: the audio to evaluate\n        channel_reference: the `nchannel` of the reference audio if it is stereo\n        channel_to_evaluate: the `nchannel` of the audio to evaluate if it is stereo\n        calibration: the calibration to use\n\n    \"\"\"\n\n    audio_reference: str | BytesIO\n    audio_to_evaluate: str | BytesIO\n    channel_reference: int = 1\n    channel_to_evaluate: int = 1\n    calibration: str = \"telephone-channel\"\n\n    @field_validator(\"audio_reference\")\n    def audio_ref_must_be_str_or_bytesio(cls, value: object):\n        if not isinstance(value, (str, BytesIO)):\n            error = \"audio must be a string or a BytesIO object\"\n            raise TypeError(error)\n        return value\n\n    @field_validator(\"audio_to_evaluate\")\n    def audio_to_eval_must_be_str_or_bytesio(cls, value: object):\n        if not isinstance(value, (str, BytesIO)):\n            error = \"audio must be a string or a BytesIO object\"\n            raise TypeError(error)\n        return value\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.SimilarityWav2WavOutput","title":"<code>SimilarityWav2WavOutput</code>","text":"<p>               Bases: <code>DaspeakResponse</code></p> <p>Output class for the similarity wav to wav endpoint.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>the similarity score between the two audios</p> <code>model</code> <code>ModelMetadata</code> <p>the model used to generate the credential</p> <code>calibration</code> <code>str</code> <p>the calibration used</p> <code>authenticity_reference</code> <code>float</code> <p>the authenticity of the reference audio sample</p> <code>authenticity_to_evaluate</code> <code>float</code> <p>the authenticity of the audio to evaluate</p> <code>input_audio_duration_reference</code> <code>float</code> <p>the duration of the reference audio</p> <code>input_audio_duration_to_evaluate</code> <code>float</code> <p>the duration of the audio to evaluate</p> <code>net_speech_duration_reference</code> <code>float</code> <p>the duration of the speech in the reference audio</p> <code>net_speech_duration_to_evaluate</code> <code>float</code> <p>the duration of the speech in the audio to evaluate</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class SimilarityWav2WavOutput(DaspeakResponse):\n    \"\"\"Output class for the similarity wav to wav endpoint.\n\n    Attributes:\n        score: the similarity score between the two audios\n        model: the model used to generate the credential\n        calibration: the calibration used\n        authenticity_reference: the authenticity of the reference audio sample\n        authenticity_to_evaluate: the authenticity of the audio to evaluate\n        input_audio_duration_reference: the duration of the reference audio\n        input_audio_duration_to_evaluate: the duration of the audio to evaluate\n        net_speech_duration_reference: the duration of the speech in the reference audio\n        net_speech_duration_to_evaluate: the duration of the speech in the audio to evaluate\n\n    \"\"\"\n\n    score: float\n    model: ModelMetadata\n    calibration: str\n    authenticity_reference: float\n    authenticity_to_evaluate: float\n    input_audio_duration_reference: float\n    input_audio_duration_to_evaluate: float\n    net_speech_duration_reference: float\n    net_speech_duration_to_evaluate: float\n</code></pre>"}]}
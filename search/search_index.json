{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Vericlient","text":"<p>Vericlient is a Python library designed to facilitate interaction with the Veridas API.</p>"},{"location":"#installation","title":"Installation","text":"<p>Installation is as simple as:</p> <pre><code>pip install vericlient\n</code></pre>"},{"location":"#api-documentation","title":"API Documentation","text":"<p>Visit the API Documentation to learn how to use the library.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>--8&lt;-- \"../CHANGELOG.md\"</p>"},{"location":"api_docs/vericlient/","title":"API Documentation","text":"<p>The <code>vericlient</code> library provides a way to interact with the Veridas APIs.</p> <p>To know how to configure the library, the following concepts must be understood:</p> <ul> <li><code>target</code>: the target of the client, which can be <code>cloud</code> or <code>custom</code>.   The <code>cloud</code> target is used to interact with the Veridas cloud APIs.   The <code>custom</code> target is used to interact with self-hosted or on-premises   Veridas APIs.</li> </ul> <p>If you want to use it with the <code>custom</code> target, don't need to care about the   rest of the parameters, but you must provide the <code>url</code>.</p> <p>Default: <code>cloud</code>.</p> <ul> <li> <p><code>url</code>: the URL of the API to interact with. It is only used when the <code>target</code>   is <code>custom</code>.</p> </li> <li> <p><code>apikey</code>: the API key to use for the requests against the Veridas Cloud API.</p> </li> <li> <p><code>environment</code>: the environment to use for the requests. It can be <code>production</code>   or <code>sandbox</code>.</p> </li> </ul> <p>Default: <code>sandbox</code>.</p> <ul> <li><code>location</code>: the location to use for the requests. It can be <code>eu</code> or <code>us</code>,   depending if the server is located in Europe or the United States.</li> </ul> <p>Default: <code>eu</code>.</p> <ul> <li><code>timeout</code>: the timeout for the requests in seconds.</li> </ul> <p>Default: <code>10</code>.</p>"},{"location":"api_docs/vericlient/#getting-started","title":"Getting started","text":"<p>The entrypoint of the library will offer all the clients available to interact with the Veridas APIs. Example:</p> <pre><code>from vericlient import DaspeakClient, DasfaceClient\n\ndaspeak_client = DaspeakClient()\ndasface_client = DasfaceClient()\n</code></pre>"},{"location":"api_docs/vericlient/#configuration","title":"Configuration","text":"<p>The library can be configured both programmatically and using environment variables.</p>"},{"location":"api_docs/vericlient/#programmatically","title":"Programmatically","text":"<p>Simply pass the desired configuration parameters to the client constructor.</p> <pre><code>from vericlient import DaspeakClient\n\nclient = DaspeakClient(\n    target=\"cloud\",\n    apikey=\"your_api_key\",\n    environment=\"sandbox\",\n    location=\"eu\",\n    timeout=10,\n)\n</code></pre> <p>Remember that there are some default values for the parameters, so you don't need to provide all of them.</p> <p>For example, if you want to use the sandbox European server, you can create the client like this:</p> <pre><code>from vericlient import DaspeakClient\n\nclient = DaspeakClient(apikey=\"your_api_key\")\n</code></pre>"},{"location":"api_docs/vericlient/#environment-variables","title":"Environment variables","text":"<p>The following environment variables are supported and will override the programmatic configuration:</p> <ul> <li><code>VERICLIENT_TARGET</code>: The target API to use.</li> <li><code>VERICLIENT_ENVIRONMENT</code>: The environment to use for the requests.</li> <li><code>VERICLIENT_APIKEY</code>: The API key to use for the requests against the Veridas Cloud API.</li> <li><code>VERICLIENT_LOCATION</code>: The location to use for the requests.</li> <li><code>VERICLIENT_URL</code>: In case you want to use a self-hosted API, you can set the URL with this variable.</li> <li><code>VERICLIENT_TIMEOUT</code>: The timeout for the requests.</li> </ul>"},{"location":"api_docs/daspeak/client/","title":"Daspeak Client","text":"<p>This is an example of how to use the Daspeak client:</p> <pre><code>from vericlient import DaspeakClient\nfrom vericlient.daspeak.models import (\n    GenerateCredentialInput,\n    CompareCredential2AudioInput,\n    CompareAudio2AudioInput,\n    CompareCredential2CredentialInput,\n    CompareAudio2CredentialsInput\n)\n\nclient = DaspeakClient(apikey=\"your_api_key\")\n\n# check if the server is alive\nprint(f\"Alive: {client.alive()}\")\n\n# get the available biometrics models\nprint(f\"Biometrics models: {client.get_models().models}\")\n\n# generate a credential from an audio file using the last model\nmodel_input = GenerateCredentialInput(\n    audio=\"/home/audio.wav\",\n    hash=client.get_models().models[-1],\n)\ngenerate_credential_output = client.generate_credential(model_input)\nprint(f\"Credential generated with an audio file: {generate_credential_output.credential}\")\n\n# generate a credential from a BytesIO object using the last model\nwith open(\"/home/audio.wav\", \"rb\") as f:\n    model_input = GenerateCredentialInput(\n        audio=f.read(),\n        hash=client.get_models().models[-1],\n    )\ngenerate_credential_output = client.generate_credential(model_input)\nprint(f\"Credential generated with virtual file: {generate_credential_output.credential}\")\n\n# compare a credential with an audio file\ncompare_input = CompareCredential2AudioInput(\n    audio_to_evaluate=\"/home/audio.wav\",\n    credential_reference=generate_credential_output.credential,\n)\ncompare_output = client.compare(compare_input)\nprint(f\"Similarity between the credential and the audio file: {compare_output.score}\")\nprint(f\"Authenticity of the audio file: {compare_output.authenticity_to_evaluate}\")\nprint(f\"Net speech duration of the audio file: {compare_output.net_speech_duration_to_evaluate}\")\n\n# compare a credential with a BytesIO object\nwith open(\"/home/audio.wav\", \"rb\") as f:\n    compare_input = CompareCredential2AudioInput(\n        audio_to_evaluate=f.read(),\n        credential_reference=generate_credential_output.credential,\n    )\ncompare_output = client.compare(compare_input)\nprint(f\"Similarity between the credential and the virtual file: {compare_output.score}\")\nprint(f\"Authenticity of the virtual file: {compare_output.authenticity_to_evaluate}\")\nprint(f\"Net speech duration of the virtual file: {compare_output.net_speech_duration_to_evaluate}\")\n\n# compare two audio files, no matter if they are virtual or real\nwith open (\"/home/audio.wav\", \"rb\") as f:\n    compare_input = CompareAudio2AudioInput(\n        audio_reference=\"/home/audio.wav\",\n        audio_to_evaluate=f.read(),\n    )\ncompare_output = client.compare(compare_input)\nprint(f\"Similarity between the two audio files: {compare_output.score}\")\nprint(f\"Authenticity of the audio file reference: {compare_output.authenticity_reference}\")\nprint(f\"Authenticity of the audio file to evaluate: {compare_output.authenticity_to_evaluate}\")\n\n# compare two credentials\ncompare_input = CompareCredential2CredentialInput(\n    credential_reference=generate_credential_output.credential,\n    credential_to_evaluate=generate_credential_output.credential,\n)\ncompare_output = client.compare(compare_input)\nprint(f\"Similarity between the two credentials: {compare_output.score}\")\n\n# identify a subject comparing an audio againts a list of credentials\ncompare_input = CompareAudio2CredentialsInput(\n    audio_reference=\"/home/audio.wav\",\n    credential_list=[\n        (\"subject1_credential\", generate_credential_output.credential),\n        (\"subject2_credential\", generate_credential_output.credential),   \n    ],\n)\ncompare_output = client.compare(compare_input)\nprint(f\"Subject identified: {compare_output.scores}\")\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient","title":"<code>vericlient.daspeak.client.DaspeakClient</code>","text":"<p>               Bases: <code>Client</code></p> <p>Class to interact with the Daspeak API.</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>class DaspeakClient(Client):\n    \"\"\"Class to interact with the Daspeak API.\"\"\"\n\n    def __init__(\n            self,\n            api: str = APIs.DASPEAK.value,\n            apikey: str | None = None,\n            target: str | None = None,\n            timeout: int | None = None,\n            environment: str | None = None,\n            location: str | None = None,\n            url: str | None = None,\n            headers: dict | None = None,\n    ) -&gt; None:\n        \"\"\"Create the DaspeakClient class.\n\n        Args:\n            api: The API to use\n            apikey: The API key to use\n            target: The target to use\n            timeout: The timeout to use in the requests\n            environment: The environment to use\n            location: The location to use\n            url: The URL to use in case of a custom target\n            headers: The headers to be used in the requests\n\n        \"\"\"\n        super().__init__(\n            api=api,\n            apikey=apikey,\n            target=target,\n            timeout=timeout,\n            environment=environment,\n            location=location,\n            url=url,\n            headers=headers,\n        )\n        self._exceptions = [\n            \"AudioInputException\",\n            \"SignalNoiseRatioException\",\n            \"VoiceDurationIsNotEnoughException\",\n            \"InvalidChannelException\",\n            \"InsufficientQuality\",\n            \"CalibrationNotAvailable\",\n            \"ServerError\",\n            \"InvalidCredential\",\n            \"UnsupportedMediaType\",\n        ]\n        self._compare_functions_map = {\n            CompareCredential2AudioInput: self._compare_credential2audio,\n            CompareAudio2AudioInput: self._compare_audio2audio,\n            CompareCredential2CredentialInput: self._compare_credential2credential,\n            CompareAudio2CredentialsInput: self._compare_audio2credentials,\n        }\n\n    def alive(self) -&gt; bool:\n        \"\"\"Check if the service is alive.\n\n        Returns\n            bool: True if the service is alive, False otherwise\n\n        \"\"\"\n        response = self._get(endpoint=DaspeakEndpoints.ALIVE.value)\n        accepted_status_code = 200\n        return response.status_code == accepted_status_code\n\n    def _handle_error_response(self, response: Response) -&gt; None:   # noqa: C901, PLR0912\n        \"\"\"Handle error responses from the API.\"\"\"\n        response_json = response.json()\n        if response_json[\"exception\"] not in self._exceptions:\n            self._raise_server_error(response)\n        if response_json[\"exception\"] == \"AudioInputException\":\n            if \"more channels than\" in response_json[\"error\"]:\n                raise TooManyAudioChannelsError\n            if \"unsupported codec\" in response_json[\"error\"]:\n                raise UnsupportedAudioCodecError\n            if \"sample rate\" in response_json[\"error\"]:\n                raise UnsupportedSampleRateError\n            if \"duration is longer\" in response_json[\"error\"]:\n                raise AudioDurationTooLongError\n            raise ValueError(response_json[\"error\"])\n        if response_json[\"exception\"] == \"SignalNoiseRatioException\":\n            raise SignalNoiseRatioError\n        if response_json[\"exception\"] == \"VoiceDurationIsNotEnoughException\":\n            error = response_json[\"error\"]\n            net_speech_detected = float(error.split(\" \")[-3].replace(\"s\", \"\"))\n            raise NetSpeechDurationIsNotEnoughError(net_speech_detected)\n        if response_json[\"exception\"] == \"InvalidChannelException\":\n            raise InvalidSpecifiedChannelError\n        if response_json[\"exception\"] == \"InsufficientQuality\":\n            raise InsufficientQualityError\n        if response_json[\"exception\"] == \"CalibrationNotAvailable\":\n            error = response_json[\"error\"]\n            calibration = str(error.split(\" \")[2])\n            raise CalibrationNotAvailableError(calibration)\n        if response_json[\"exception\"] == \"InvalidCredential\":\n            raise InvalidCredentialError\n        if response_json[\"exception\"] == \"UnsupportedMediaType\":\n            raise UnsupportedMediaTypeError\n        raise ValueError(response_json[\"error\"])\n\n    def get_models(self) -&gt; ModelsOutput:\n        \"\"\"Get the models available biometrics models in the service.\n\n        Returns:\n            The response from the service\n\n        \"\"\"\n        response = self._get(endpoint=DaspeakEndpoints.MODELS.value)\n        return ModelsOutput(status_code=response.status_code, **response.json())\n\n    def generate_credential(self, data_model: GenerateCredentialInput) -&gt; GenerateCredentialOutput:\n        \"\"\"Generate a credential from a WAV file.\n\n        Args:\n            data_model: The data required to generate the credential\n\n        Returns:\n            The response from the service\n\n        Raises:\n            ValueError: If the `data_model` is not an instance of `GenerateCredentialInput`\n            TooManyAudioChannelsError: If the audio has more channels than the service supports\n            UnsupportedAudioCodecError: If the audio has an unsupported codec\n            UnsupportedSampleRateError: If the audio has an unsupported sample rate\n            AudioDurationTooLongError: If the audio duration is longer than the service supports\n            SignalNoiseRatioError: If the signal-to-noise ratio is too low\n            NetSpeechDurationIsNotEnoughError: If the net speech duration is not enough\n            InvalidSpecifiedChannelError: If the specified channel is invalid\n            InsufficientQualityError: If the audio quality is insufficient\n            CalibrationNotAvailableError: If the calibration is not available\n            UnsupportedMediaTypeError: If the media type is not supported\n\n        \"\"\"\n        endpoint = DaspeakEndpoints.MODELS_HASH_CREDENTIAL_AUDIO.value.replace(\"&lt;hash&gt;\", data_model.hash)\n        audio = self._get_virtual_audio_file(data_model.audio)\n        files = {\n            \"audio\": (\"audio\", audio, \"audio/wav\"),\n        }\n        data = {\n            \"channel\": data_model.channel,\n            \"calibration\": data_model.calibration,\n        }\n        response = self._post(endpoint=endpoint, data=data, files=files)\n        return GenerateCredentialOutput(status_code=response.status_code, **response.json())\n\n    def compare(\n            self,\n            data_model: CompareInput,\n        ) -&gt; CompareCredential2AudioOutput | CompareAudio2AudioOutput | \\\n             CompareCredential2CredentialOutput | CompareAudio2CredentialsOutput :\n        \"\"\"Compare audio files or credentials.\n\n        Args:\n            data_model (CompareCredential2AudioInput | CompareAudio2AudioInput):\n                The data required to compare the audio files or credentials\n\n        Returns:\n            (CompareCredential2AudioOutput | CompareAudio2AudioOutput): The response from the service,\n                depending on the input type.\n\n        Raises:\n            ValueError: If the `data_model` is not an instance of `CompareInput`\n            TooManyAudioChannelsError: If the audio has more channels than the service supports\n            UnsupportedAudioCodecError: If the audio has an unsupported codec\n            UnsupportedSampleRateError: If the audio has an unsupported sample rate\n            AudioDurationTooLongError: If the audio duration is longer than the service supports\n            SignalNoiseRatioError: If the signal-to-noise ratio is too low\n            NetSpeechDurationIsNotEnoughError: If the net speech duration is not enough\n            InvalidSpecifiedChannelError: If the specified channel is invalid\n            InsufficientQualityError: If the audio quality is insufficient\n            CalibrationNotAvailableError: If the calibration is not available\n            InvalidCredentialError: If the credential is invalid\n            UnsupportedMediaTypeError: If the media type is not supported\n\n        \"\"\"\n        try:\n            func = self._compare_functions_map.get(type(data_model))\n            return func(data_model)\n        except AttributeError:\n            error = \"data_model must be an instance of CompareInput\"\n            raise ValueError(error) from None\n\n    def _compare_credential2audio(\n            self,\n            data_model: CompareCredential2AudioInput,\n        ) -&gt; CompareCredential2AudioOutput:\n        \"\"\"Compare a credential with an audio file.\n\n        Args:\n            data_model: The data required to compare the credential with the audio\n\n        Returns:\n            CompareCredential2AudioOutput: The response from the service\n\n        \"\"\"\n        endpoint = DaspeakEndpoints.SIMILARITY_CREDENTIAL2AUDIO.value\n        audio = self._get_virtual_audio_file(data_model.audio_to_evaluate)\n        files = {\n            \"audio_to_evaluate\": (\"audio\", audio, \"audio/wav\"),\n        }\n        data = {\n            \"credential_reference\": data_model.credential_reference,\n            \"channel\": data_model.channel,\n            \"calibration\": data_model.calibration,\n        }\n        response = self._post(endpoint=endpoint, data=data, files=files)\n        return CompareCredential2AudioOutput(status_code=response.status_code, **response.json())\n\n    def _compare_audio2audio(self, data_model: CompareAudio2AudioInput) -&gt; CompareAudio2AudioOutput:\n        \"\"\"Compare two audio files.\n\n        Args:\n            data_model: The data required to compare the audio files\n\n        Returns:\n            CompareAudio2AudioOutput: The response from the service\n\n        \"\"\"\n        endpoint = DaspeakEndpoints.SIMILARITY_AUDIO2AUDIO.value\n        audio_reference = self._get_virtual_audio_file(data_model.audio_reference)\n        audio_to_evaluate = self._get_virtual_audio_file(data_model.audio_to_evaluate)\n        files = {\n            \"audio_reference\": (\"audio\", audio_reference, \"audio/wav\"),\n            \"audio_to_evaluate\": (\"audio\", audio_to_evaluate, \"audio/wav\"),\n        }\n        data = {\n            \"channel_reference\": data_model.channel_reference,\n            \"channel_to_evaluate\": data_model.channel_to_evaluate,\n            \"calibration\": data_model.calibration,\n        }\n        response = self._post(endpoint=endpoint, data=data, files=files)\n        return CompareAudio2AudioOutput(status_code=response.status_code, **response.json())\n\n    def _compare_credential2credential(\n            self,\n            data_model: CompareCredential2CredentialInput,\n        ) -&gt; CompareCredential2CredentialOutput:\n        \"\"\"Compare two credentials.\n\n        Args:\n            data_model: The data required to compare the credentials\n\n        Returns:\n            CompareCredential2CredentialOutput: The response from the service\n\n        \"\"\"\n        endpoint = DaspeakEndpoints.SIMILARITY_CREDENTIAL2CREDENTIAL.value\n        data = {\n            \"credential_reference\": data_model.credential_reference,\n            \"credential_to_evaluate\": data_model.credential_to_evaluate,\n            \"calibration\": data_model.calibration,\n        }\n        response = self._post(endpoint=endpoint, data=data)\n        return CompareCredential2CredentialOutput(status_code=response.status_code, **response.json())\n\n    def _compare_audio2credentials(\n        self,\n        data_model: CompareAudio2CredentialsInput,\n    ) -&gt; CompareAudio2CredentialsOutput:\n        \"\"\"Compare an audio file with a list of credentials.\n\n        Args:\n            data_model: The data required to compare the audio file with the credentials\n\n        Returns:\n            CompareAudio2CredentialsOutput: The response from the service\n\n        \"\"\"\n        endpoint = DaspeakEndpoints.IDENTIFICATION_AUDIO2CREDENTIALS.value\n        audio = self._get_virtual_audio_file(data_model.audio_reference)\n        files = {\n            \"audio_reference\": (\"audio_reference\", audio, \"audio/wav\"),\n        }\n        credential_list = json.dumps(data_model.credential_list)\n        data = {\n            \"credential_list\": credential_list,\n            \"channel\": data_model.channel,\n            \"calibration\": data_model.calibration,\n        }\n        response = self._post(endpoint=endpoint, data=data, files=files)\n        return CompareAudio2CredentialsOutput(status_code=response.status_code, **response.json())\n\n    def _get_virtual_audio_file(self, audio_input: object) -&gt; bytes:\n        if isinstance(audio_input, str):\n            try:\n                with open(audio_input, \"rb\") as f:\n                    audio = f.read()\n            except FileNotFoundError as e:\n                error = f\"File {audio_input} not found\"\n                raise FileNotFoundError(error) from e\n        elif isinstance(audio_input, bytes):\n            audio = audio_input\n        else:\n            error = \"audio must be a string or a bytes object\"\n            raise TypeError(error)\n        return audio\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient.__init__","title":"<code>__init__(api=APIs.DASPEAK.value, apikey=None, target=None, timeout=None, environment=None, location=None, url=None, headers=None)</code>","text":"<p>Create the DaspeakClient class.</p> <p>Parameters:</p> Name Type Description Default <code>api</code> <code>str</code> <p>The API to use</p> <code>DASPEAK.value</code> <code>apikey</code> <code>str | None</code> <p>The API key to use</p> <code>None</code> <code>target</code> <code>str | None</code> <p>The target to use</p> <code>None</code> <code>timeout</code> <code>int | None</code> <p>The timeout to use in the requests</p> <code>None</code> <code>environment</code> <code>str | None</code> <p>The environment to use</p> <code>None</code> <code>location</code> <code>str | None</code> <p>The location to use</p> <code>None</code> <code>url</code> <code>str | None</code> <p>The URL to use in case of a custom target</p> <code>None</code> <code>headers</code> <code>dict | None</code> <p>The headers to be used in the requests</p> <code>None</code> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def __init__(\n        self,\n        api: str = APIs.DASPEAK.value,\n        apikey: str | None = None,\n        target: str | None = None,\n        timeout: int | None = None,\n        environment: str | None = None,\n        location: str | None = None,\n        url: str | None = None,\n        headers: dict | None = None,\n) -&gt; None:\n    \"\"\"Create the DaspeakClient class.\n\n    Args:\n        api: The API to use\n        apikey: The API key to use\n        target: The target to use\n        timeout: The timeout to use in the requests\n        environment: The environment to use\n        location: The location to use\n        url: The URL to use in case of a custom target\n        headers: The headers to be used in the requests\n\n    \"\"\"\n    super().__init__(\n        api=api,\n        apikey=apikey,\n        target=target,\n        timeout=timeout,\n        environment=environment,\n        location=location,\n        url=url,\n        headers=headers,\n    )\n    self._exceptions = [\n        \"AudioInputException\",\n        \"SignalNoiseRatioException\",\n        \"VoiceDurationIsNotEnoughException\",\n        \"InvalidChannelException\",\n        \"InsufficientQuality\",\n        \"CalibrationNotAvailable\",\n        \"ServerError\",\n        \"InvalidCredential\",\n        \"UnsupportedMediaType\",\n    ]\n    self._compare_functions_map = {\n        CompareCredential2AudioInput: self._compare_credential2audio,\n        CompareAudio2AudioInput: self._compare_audio2audio,\n        CompareCredential2CredentialInput: self._compare_credential2credential,\n        CompareAudio2CredentialsInput: self._compare_audio2credentials,\n    }\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient.alive","title":"<code>alive()</code>","text":"<p>Check if the service is alive.</p> <p>Returns     bool: True if the service is alive, False otherwise</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def alive(self) -&gt; bool:\n    \"\"\"Check if the service is alive.\n\n    Returns\n        bool: True if the service is alive, False otherwise\n\n    \"\"\"\n    response = self._get(endpoint=DaspeakEndpoints.ALIVE.value)\n    accepted_status_code = 200\n    return response.status_code == accepted_status_code\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient.compare","title":"<code>compare(data_model)</code>","text":"<p>Compare audio files or credentials.</p> <p>Parameters:</p> Name Type Description Default <code>data_model</code> <code>CompareCredential2AudioInput | CompareAudio2AudioInput</code> <p>The data required to compare the audio files or credentials</p> required <p>Returns:</p> Type Description <code>CompareCredential2AudioOutput | CompareAudio2AudioOutput</code> <p>The response from the service, depending on the input type.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>data_model</code> is not an instance of <code>CompareInput</code></p> <code>TooManyAudioChannelsError</code> <p>If the audio has more channels than the service supports</p> <code>UnsupportedAudioCodecError</code> <p>If the audio has an unsupported codec</p> <code>UnsupportedSampleRateError</code> <p>If the audio has an unsupported sample rate</p> <code>AudioDurationTooLongError</code> <p>If the audio duration is longer than the service supports</p> <code>SignalNoiseRatioError</code> <p>If the signal-to-noise ratio is too low</p> <code>NetSpeechDurationIsNotEnoughError</code> <p>If the net speech duration is not enough</p> <code>InvalidSpecifiedChannelError</code> <p>If the specified channel is invalid</p> <code>InsufficientQualityError</code> <p>If the audio quality is insufficient</p> <code>CalibrationNotAvailableError</code> <p>If the calibration is not available</p> <code>InvalidCredentialError</code> <p>If the credential is invalid</p> <code>UnsupportedMediaTypeError</code> <p>If the media type is not supported</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def compare(\n        self,\n        data_model: CompareInput,\n    ) -&gt; CompareCredential2AudioOutput | CompareAudio2AudioOutput | \\\n         CompareCredential2CredentialOutput | CompareAudio2CredentialsOutput :\n    \"\"\"Compare audio files or credentials.\n\n    Args:\n        data_model (CompareCredential2AudioInput | CompareAudio2AudioInput):\n            The data required to compare the audio files or credentials\n\n    Returns:\n        (CompareCredential2AudioOutput | CompareAudio2AudioOutput): The response from the service,\n            depending on the input type.\n\n    Raises:\n        ValueError: If the `data_model` is not an instance of `CompareInput`\n        TooManyAudioChannelsError: If the audio has more channels than the service supports\n        UnsupportedAudioCodecError: If the audio has an unsupported codec\n        UnsupportedSampleRateError: If the audio has an unsupported sample rate\n        AudioDurationTooLongError: If the audio duration is longer than the service supports\n        SignalNoiseRatioError: If the signal-to-noise ratio is too low\n        NetSpeechDurationIsNotEnoughError: If the net speech duration is not enough\n        InvalidSpecifiedChannelError: If the specified channel is invalid\n        InsufficientQualityError: If the audio quality is insufficient\n        CalibrationNotAvailableError: If the calibration is not available\n        InvalidCredentialError: If the credential is invalid\n        UnsupportedMediaTypeError: If the media type is not supported\n\n    \"\"\"\n    try:\n        func = self._compare_functions_map.get(type(data_model))\n        return func(data_model)\n    except AttributeError:\n        error = \"data_model must be an instance of CompareInput\"\n        raise ValueError(error) from None\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient.generate_credential","title":"<code>generate_credential(data_model)</code>","text":"<p>Generate a credential from a WAV file.</p> <p>Parameters:</p> Name Type Description Default <code>data_model</code> <code>GenerateCredentialInput</code> <p>The data required to generate the credential</p> required <p>Returns:</p> Type Description <code>GenerateCredentialOutput</code> <p>The response from the service</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>data_model</code> is not an instance of <code>GenerateCredentialInput</code></p> <code>TooManyAudioChannelsError</code> <p>If the audio has more channels than the service supports</p> <code>UnsupportedAudioCodecError</code> <p>If the audio has an unsupported codec</p> <code>UnsupportedSampleRateError</code> <p>If the audio has an unsupported sample rate</p> <code>AudioDurationTooLongError</code> <p>If the audio duration is longer than the service supports</p> <code>SignalNoiseRatioError</code> <p>If the signal-to-noise ratio is too low</p> <code>NetSpeechDurationIsNotEnoughError</code> <p>If the net speech duration is not enough</p> <code>InvalidSpecifiedChannelError</code> <p>If the specified channel is invalid</p> <code>InsufficientQualityError</code> <p>If the audio quality is insufficient</p> <code>CalibrationNotAvailableError</code> <p>If the calibration is not available</p> <code>UnsupportedMediaTypeError</code> <p>If the media type is not supported</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def generate_credential(self, data_model: GenerateCredentialInput) -&gt; GenerateCredentialOutput:\n    \"\"\"Generate a credential from a WAV file.\n\n    Args:\n        data_model: The data required to generate the credential\n\n    Returns:\n        The response from the service\n\n    Raises:\n        ValueError: If the `data_model` is not an instance of `GenerateCredentialInput`\n        TooManyAudioChannelsError: If the audio has more channels than the service supports\n        UnsupportedAudioCodecError: If the audio has an unsupported codec\n        UnsupportedSampleRateError: If the audio has an unsupported sample rate\n        AudioDurationTooLongError: If the audio duration is longer than the service supports\n        SignalNoiseRatioError: If the signal-to-noise ratio is too low\n        NetSpeechDurationIsNotEnoughError: If the net speech duration is not enough\n        InvalidSpecifiedChannelError: If the specified channel is invalid\n        InsufficientQualityError: If the audio quality is insufficient\n        CalibrationNotAvailableError: If the calibration is not available\n        UnsupportedMediaTypeError: If the media type is not supported\n\n    \"\"\"\n    endpoint = DaspeakEndpoints.MODELS_HASH_CREDENTIAL_AUDIO.value.replace(\"&lt;hash&gt;\", data_model.hash)\n    audio = self._get_virtual_audio_file(data_model.audio)\n    files = {\n        \"audio\": (\"audio\", audio, \"audio/wav\"),\n    }\n    data = {\n        \"channel\": data_model.channel,\n        \"calibration\": data_model.calibration,\n    }\n    response = self._post(endpoint=endpoint, data=data, files=files)\n    return GenerateCredentialOutput(status_code=response.status_code, **response.json())\n</code></pre>"},{"location":"api_docs/daspeak/client/#vericlient.daspeak.client.DaspeakClient.get_models","title":"<code>get_models()</code>","text":"<p>Get the models available biometrics models in the service.</p> <p>Returns:</p> Type Description <code>ModelsOutput</code> <p>The response from the service</p> Source code in <code>src/vericlient/daspeak/client.py</code> <pre><code>def get_models(self) -&gt; ModelsOutput:\n    \"\"\"Get the models available biometrics models in the service.\n\n    Returns:\n        The response from the service\n\n    \"\"\"\n    response = self._get(endpoint=DaspeakEndpoints.MODELS.value)\n    return ModelsOutput(status_code=response.status_code, **response.json())\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/","title":"Daspeak Exceptions","text":""},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions","title":"<code>vericlient.daspeak.exceptions</code>","text":"<p>Module to define the exceptions for the Daspeak API.</p>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.AudioDurationTooLongError","title":"<code>AudioDurationTooLongError</code>","text":"<p>               Bases: <code>AudioInputError</code></p> <p>Exception raised for audio duration too long.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class AudioDurationTooLongError(AudioInputError):\n    \"\"\"Exception raised for audio duration too long.\"\"\"\n\n    def __init__(self) -&gt; None:\n        message = \"The audio duration is too long, must be less than 30 seconds\"\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.AudioInputError","title":"<code>AudioInputError</code>","text":"<p>               Bases: <code>DaspeakError</code></p> <p>Exception raised for errors in the audio input.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class AudioInputError(DaspeakError):\n    \"\"\"Exception raised for errors in the audio input.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.CalibrationNotAvailableError","title":"<code>CalibrationNotAvailableError</code>","text":"<p>               Bases: <code>DaspeakError</code></p> <p>Exception raised for calibration not available.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class CalibrationNotAvailableError(DaspeakError):\n    \"\"\"Exception raised for calibration not available.\"\"\"\n\n    def __init__(self, calibration: str) -&gt; None:\n        message = f\"The calibration {calibration} is not available\"\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.DaspeakError","title":"<code>DaspeakError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for exceptions in the Daspeak API.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class DaspeakError(Exception):\n    \"\"\"Base class for exceptions in the Daspeak API.\"\"\"\n\n    def __init__(self, message: str) -&gt; None:\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.InsufficientQualityError","title":"<code>InsufficientQualityError</code>","text":"<p>               Bases: <code>DaspeakError</code></p> <p>Exception raised for insufficient quality.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class InsufficientQualityError(DaspeakError):\n    \"\"\"Exception raised for insufficient quality.\"\"\"\n\n    def __init__(self) -&gt; None:\n        message = \"The audio quality is insufficient or may contain more than one speaker\"\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.InvalidSpecifiedChannelError","title":"<code>InvalidSpecifiedChannelError</code>","text":"<p>               Bases: <code>DaspeakError</code></p> <p>Exception raised for invalid specified channel.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class InvalidSpecifiedChannelError(DaspeakError):\n    \"\"\"Exception raised for invalid specified channel.\"\"\"\n\n    def __init__(self) -&gt; None:\n        message = \"The specified channel is invalid, must be 1 or 2\"\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.NetSpeechDurationIsNotEnoughError","title":"<code>NetSpeechDurationIsNotEnoughError</code>","text":"<p>               Bases: <code>DaspeakError</code></p> <p>Exception raised for errors in the net speech duration.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class NetSpeechDurationIsNotEnoughError(DaspeakError):\n    \"\"\"Exception raised for errors in the net speech duration.\"\"\"\n\n    def __init__(self, net_speech_detected: float) -&gt; None:\n        message = (\n            f\"You need at least 3 seconds of speech to perform the operation, \"\n            f\"but only {net_speech_detected} seconds were detected\"\n        )\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.SignalNoiseRatioError","title":"<code>SignalNoiseRatioError</code>","text":"<p>               Bases: <code>DaspeakError</code></p> <p>Exception raised for errors in the signal noise ratio.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class SignalNoiseRatioError(DaspeakError):\n    \"\"\"Exception raised for errors in the signal noise ratio.\"\"\"\n\n    def __init__(self) -&gt; None:\n        message = \"Noise level of the audio exceeded\"\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.TooManyAudioChannelsError","title":"<code>TooManyAudioChannelsError</code>","text":"<p>               Bases: <code>AudioInputError</code></p> <p>Exception raised for too many audio channels.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class TooManyAudioChannelsError(AudioInputError):\n    \"\"\"Exception raised for too many audio channels.\"\"\"\n\n    def __init__(self) -&gt; None:\n        message = \"The maximum allowed number of audio channels is 2, and the audio provided has more channels\"\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.UnsupportedAudioCodecError","title":"<code>UnsupportedAudioCodecError</code>","text":"<p>               Bases: <code>AudioInputError</code></p> <p>Exception raised for unsupported audio codec.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class UnsupportedAudioCodecError(AudioInputError):\n    \"\"\"Exception raised for unsupported audio codec.\"\"\"\n\n    def __init__(self) -&gt; None:\n        message = \"The audio codec is not supported. Supported codecs are: 'PCM_16', 'ULAW', 'ALAW'\"\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/exceptions/#vericlient.daspeak.exceptions.UnsupportedSampleRateError","title":"<code>UnsupportedSampleRateError</code>","text":"<p>               Bases: <code>AudioInputError</code></p> <p>Exception raised for unsupported sample rates.</p> Source code in <code>src/vericlient/daspeak/exceptions.py</code> <pre><code>class UnsupportedSampleRateError(AudioInputError):\n    \"\"\"Exception raised for unsupported sample rates.\"\"\"\n\n    def __init__(self) -&gt; None:\n        message = \"The sample rate of the audio is not supported, must be 8 Khz or 16 Khz\"\n        super().__init__(message)\n</code></pre>"},{"location":"api_docs/daspeak/models/","title":"Daspeak Models","text":""},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models","title":"<code>vericlient.daspeak.models</code>","text":"<p>Module to define the models for the Daspeak API.</p>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.CompareAudio2AudioInput","title":"<code>CompareAudio2AudioInput</code>","text":"<p>               Bases: <code>CompareInput</code></p> <p>Input class for the similarity audio to audio endpoint.</p> <p>Attributes:</p> Name Type Description <code>audio_reference</code> <code>str | bytes</code> <p>The reference audio. It can be a path to a file or a bytes object with the audio content</p> <code>audio_to_evaluate</code> <code>str | bytes</code> <p>The audio to evaluate. It can be a path to a file or a bytes object with the audio content</p> <code>channel_reference</code> <code>int</code> <p>The <code>nchannel</code> of the reference audio if it is stereo</p> <code>channel_to_evaluate</code> <code>int</code> <p>The <code>nchannel</code> of the audio to evaluate if it is stereo</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class CompareAudio2AudioInput(CompareInput):\n    \"\"\"Input class for the similarity audio to audio endpoint.\n\n    Attributes:\n        audio_reference: The reference audio.\n            It can be a path to a file or a bytes object\n            with the audio content\n        audio_to_evaluate: The audio to evaluate.\n            It can be a path to a file or a bytes object\n            with the audio content\n        channel_reference: The `nchannel` of the reference audio if it is stereo\n        channel_to_evaluate: The `nchannel` of the audio to evaluate if it is stereo\n\n    \"\"\"\n\n    audio_reference: str | bytes\n    audio_to_evaluate: str | bytes\n    channel_reference: int = 1\n    channel_to_evaluate: int = 1\n\n    @field_validator(\"audio_reference\", \"audio_to_evaluate\")\n    def audio_ref_must_be_str_or_bytes(cls, value: object):\n        if not isinstance(value, (str, bytes)):\n            error = \"audio must be a string or a bytes object\"\n            raise TypeError(error)\n        return value\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.CompareAudio2AudioOutput","title":"<code>CompareAudio2AudioOutput</code>","text":"<p>               Bases: <code>CompareOutput</code></p> <p>Output class for the similarity audio to audio endpoint.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>ModelMetadata</code> <p>The model used to generate the credential</p> <code>calibration</code> <code>str</code> <p>The calibration used</p> <code>authenticity_reference</code> <code>float</code> <p>The authenticity of the reference audio sample</p> <code>authenticity_to_evaluate</code> <code>float</code> <p>The authenticity of the audio to evaluate</p> <code>input_audio_duration_reference</code> <code>float</code> <p>The duration of the reference audio</p> <code>input_audio_duration_to_evaluate</code> <code>float</code> <p>The duration of the audio to evaluate</p> <code>net_speech_duration_reference</code> <code>float</code> <p>The duration of the speech in the reference audio</p> <code>net_speech_duration_to_evaluate</code> <code>float</code> <p>The duration of the speech in the audio to evaluate</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class CompareAudio2AudioOutput(CompareOutput):\n    \"\"\"Output class for the similarity audio to audio endpoint.\n\n    Attributes:\n        model: The model used to generate the credential\n        calibration: The calibration used\n        authenticity_reference: The authenticity of the reference audio sample\n        authenticity_to_evaluate: The authenticity of the audio to evaluate\n        input_audio_duration_reference: The duration of the reference audio\n        input_audio_duration_to_evaluate: The duration of the audio to evaluate\n        net_speech_duration_reference: The duration of the speech in the reference audio\n        net_speech_duration_to_evaluate: The duration of the speech in the audio to evaluate\n\n    \"\"\"\n\n    model: ModelMetadata\n    calibration: str\n    authenticity_reference: float\n    authenticity_to_evaluate: float\n    input_audio_duration_reference: float\n    input_audio_duration_to_evaluate: float\n    net_speech_duration_reference: float\n    net_speech_duration_to_evaluate: float\n\n    @field_validator(\"authenticity_reference\", \"authenticity_to_evaluate\")\n    def round_value(cls, value: float) -&gt; float:\n        return round(value, 3)\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.CompareAudio2CredentialsInput","title":"<code>CompareAudio2CredentialsInput</code>","text":"<p>               Bases: <code>CompareInput</code></p> <p>Input class for the identification audio to credentials endpoint.</p> <p>Attributes:</p> Name Type Description <code>audio_reference</code> <code>str | bytes</code> <p>The audio to evaluate. It can be a path to a file or a bytes object with the audio content</p> <code>credential_list</code> <code>list[tuple[str, str]]</code> <p>The credentials to compare the audio with. The list contains touples with two strings: the id and the credential</p> <code>channel</code> <code>int</code> <p>The <code>nchannel</code> of the audio if it is stereo</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class CompareAudio2CredentialsInput(CompareInput):\n    \"\"\"Input class for the identification audio to credentials endpoint.\n\n    Attributes:\n        audio_reference: The audio to evaluate.\n            It can be a path to a file or a bytes object\n            with the audio content\n        credential_list: The credentials to compare the audio with.\n            The list contains touples with two strings: the id and the credential\n        channel: The `nchannel` of the audio if it is stereo\n\n    \"\"\"\n\n    audio_reference: str | bytes\n    credential_list: list[tuple[str, str]]\n    channel: int = 1\n\n    @field_validator(\"audio_reference\")\n    def must_be_str_or_bytes(cls, value: object):\n        if not isinstance(value, (str, bytes)):\n            error = \"audio must be a string or a bytes object\"\n            raise TypeError(error)\n        return value\n\n    @field_validator(\"credential_list\")\n    def validate_and_build_list_format(cls, value: list):\n        if not value:\n            error = \"credential_list must not be empty\"\n            raise ValueError(error)\n        error = \"credential_list must contain touples with two strings\"\n        n_items = 2\n        for item in value:\n            if not isinstance(item, tuple) or len(item) != n_items:\n                raise ValueError(error)\n            if not all(isinstance(i, str) for i in item):\n                raise ValueError(error)\n        return [{\"id\": item[0], \"credential\": item[1]} for item in value]\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.CompareAudio2CredentialsOutput","title":"<code>CompareAudio2CredentialsOutput</code>","text":"<p>               Bases: <code>DaspeakResponse</code></p> <p>Output class for the identification audio to credentials endpoint.</p> <p>Attributes:</p> Name Type Description <code>result</code> <code>dict</code> <p>The result of the identification, a dictionary with the \"id\" and the \"score\" of the best match</p> <code>scores</code> <code>list[dict]</code> <p>The whole list of scores for each credential. The list contains dictionaries with two keys (and values): \"id\" and \"score\"</p> <code>calibration</code> <code>str</code> <p>The calibration used</p> <code>model</code> <code>ModelMetadata</code> <p>The model used to generate the credential</p> <code>authenticity_reference</code> <code>float</code> <p>The authenticity of the reference audio sample</p> <code>input_audio_duration_reference</code> <code>float</code> <p>The duration of the input audio</p> <code>net_speech_duration_reference</code> <code>float</code> <p>The duration of the speech in the audio</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class CompareAudio2CredentialsOutput(DaspeakResponse):\n    \"\"\"Output class for the identification audio to credentials endpoint.\n\n    Attributes:\n        result: The result of the identification, a dictionary with the \"id\" and the \"score\"\n            of the best match\n        scores: The whole list of scores for each credential.\n            The list contains dictionaries with two keys (and values): \"id\" and \"score\"\n        calibration: The calibration used\n        model: The model used to generate the credential\n        authenticity_reference: The authenticity of the reference audio sample\n        input_audio_duration_reference: The duration of the input audio\n        net_speech_duration_reference: The duration of the speech in the audio\n\n    \"\"\"\n\n    result: dict\n    scores: list[dict]\n    calibration: str\n    model: ModelMetadata\n    authenticity_reference: float\n    input_audio_duration_reference: float\n    net_speech_duration_reference: float\n\n    @field_validator(\"authenticity_reference\")\n    def round_value(cls, value: float) -&gt; float:\n        return round(value, 3)\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.CompareCredential2AudioInput","title":"<code>CompareCredential2AudioInput</code>","text":"<p>               Bases: <code>CompareInput</code></p> <p>Input class for the similarity credential to audio endpoint.</p> <p>Attributes:</p> Name Type Description <code>credential_reference</code> <code>str</code> <p>The reference credential</p> <code>audio_to_evaluate</code> <code>str | bytes</code> <p>The audio to evaluate. It can be a path to a file or a bytes object with the audio content</p> <code>channel</code> <code>int</code> <p>The <code>nchannel</code> of the audio if it is stereo</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class CompareCredential2AudioInput(CompareInput):\n    \"\"\"Input class for the similarity credential to audio endpoint.\n\n    Attributes:\n        credential_reference: The reference credential\n        audio_to_evaluate: The audio to evaluate.\n            It can be a path to a file or a bytes object\n            with the audio content\n        channel: The `nchannel` of the audio if it is stereo\n\n    \"\"\"\n\n    credential_reference: str\n    audio_to_evaluate: str | bytes\n    channel: int = 1\n\n    @field_validator(\"audio_to_evaluate\")\n    def must_be_str_or_bytes(cls, value: object):\n        if not isinstance(value, (str, bytes)):\n            error = \"audio must be a string or a bytes object\"\n            raise TypeError(error)\n        return value\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.CompareCredential2AudioOutput","title":"<code>CompareCredential2AudioOutput</code>","text":"<p>               Bases: <code>CompareOutput</code></p> <p>Output class for the similarity credential to audio endpoint.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>ModelMetadata</code> <p>The model used to generate the credential</p> <code>calibration</code> <code>str</code> <p>The calibration used</p> <code>authenticity_to_evaluate</code> <code>float</code> <p>The authenticity of the audio sample used</p> <code>input_audio_duration</code> <code>float</code> <p>The duration of the input audio</p> <code>net_speech_duration</code> <code>float</code> <p>The duration of the speech in the audio</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class CompareCredential2AudioOutput(CompareOutput):\n    \"\"\"Output class for the similarity credential to audio endpoint.\n\n    Attributes:\n        model: The model used to generate the credential\n        calibration: The calibration used\n        authenticity_to_evaluate: The authenticity of the audio sample used\n        input_audio_duration: The duration of the input audio\n        net_speech_duration: The duration of the speech in the audio\n\n    \"\"\"\n\n    model: ModelMetadata\n    calibration: str\n    authenticity_to_evaluate: float\n    input_audio_duration_to_evaluate: float\n    net_speech_duration_to_evaluate: float\n\n    @field_validator(\"authenticity_to_evaluate\")\n    def round_value(cls, value: float) -&gt; float:\n        return round(value, 3)\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.CompareCredential2CredentialInput","title":"<code>CompareCredential2CredentialInput</code>","text":"<p>               Bases: <code>CompareInput</code></p> <p>Input class for the similarity credential to credential endpoint.</p> <p>Attributes:</p> Name Type Description <code>credential_reference</code> <code>str</code> <p>The reference credential</p> <code>credential_to_evaluate</code> <code>str</code> <p>The credential to evaluate</p> <code>calibration</code> <code>str</code> <p>The calibration to use</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class CompareCredential2CredentialInput(CompareInput):\n    \"\"\"Input class for the similarity credential to credential endpoint.\n\n    Attributes:\n        credential_reference: The reference credential\n        credential_to_evaluate: The credential to evaluate\n        calibration: The calibration to use\n\n    \"\"\"\n\n    credential_reference: str\n    credential_to_evaluate: str\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.CompareCredential2CredentialOutput","title":"<code>CompareCredential2CredentialOutput</code>","text":"<p>               Bases: <code>CompareOutput</code></p> <p>Output class for the similarity credential to credential endpoint.</p> <p>Attributes:</p> Name Type Description <code>calibration</code> <code>str</code> <p>The calibration used</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class CompareCredential2CredentialOutput(CompareOutput):\n    \"\"\"Output class for the similarity credential to credential endpoint.\n\n    Attributes:\n        calibration: The calibration used\n\n    \"\"\"\n\n    calibration: str\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.CompareInput","title":"<code>CompareInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for the similarity inputs.</p> <p>Attributes:</p> Name Type Description <code>calibration</code> <code>str</code> <p>The calibration to use</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class CompareInput(BaseModel):\n    \"\"\"Base class for the similarity inputs.\n\n    Attributes:\n        calibration: The calibration to use\n\n    \"\"\"\n\n    calibration: str = \"telephone-channel\"\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.CompareOutput","title":"<code>CompareOutput</code>","text":"<p>               Bases: <code>DaspeakResponse</code></p> <p>Base class for the similarity outputs.</p> <p>Attributes:</p> Name Type Description <code>score</code> <code>float</code> <p>The similarity score between the two inputs</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class CompareOutput(DaspeakResponse):\n    \"\"\"Base class for the similarity outputs.\n\n    Attributes:\n        score: The similarity score between the two inputs\n\n    \"\"\"\n\n    score: float\n\n    @field_validator(\"score\")\n    def round_value(cls, value: float) -&gt; float:\n        return round(value, 3)\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.DaspeakResponse","title":"<code>DaspeakResponse</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Base class for the Daspeak API responses.</p> <p>Attributes:</p> Name Type Description <code>version</code> <code>str</code> <p>The version of the API</p> <code>status_code</code> <code>int</code> <p>The status code of the response</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class DaspeakResponse(BaseModel):\n    \"\"\"Base class for the Daspeak API responses.\n\n    Attributes:\n        version: The version of the API\n        status_code: The status code of the response\n\n    \"\"\"\n\n    version: str\n    status_code: int\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.GenerateCredentialInput","title":"<code>GenerateCredentialInput</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Input class for the generate credential endpoint.</p> <p>Attributes:</p> Name Type Description <code>audio</code> <code>str | bytes</code> <p>The audio to generate the credential with. It can be a path to a file or a bytes object with the audio content</p> <code>hash</code> <code>str</code> <p>The hash of the biometrics model to use</p> <code>channel</code> <code>int</code> <p>The <code>nchannel</code> of the audio if it is stereo</p> <code>calibration</code> <code>str</code> <p>The calibration to use</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class GenerateCredentialInput(BaseModel):\n    \"\"\"Input class for the generate credential endpoint.\n\n    Attributes:\n        audio: The audio to generate the credential with.\n            It can be a path to a file or a bytes object\n            with the audio content\n        hash: The hash of the biometrics model to use\n        channel: The `nchannel` of the audio if it is stereo\n        calibration: The calibration to use\n\n    \"\"\"\n\n    audio: str | bytes\n    hash: str\n    channel: int = 1\n    calibration: str = \"telephone-channel\"\n\n    @field_validator(\"audio\")\n    def must_be_str_or_bytes(cls, value: object):\n        if not isinstance(value, (str, bytes)):\n            error = \"audio must be a string or a bytes object\"\n            raise TypeError(error)\n        return value\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.GenerateCredentialOutput","title":"<code>GenerateCredentialOutput</code>","text":"<p>               Bases: <code>DaspeakResponse</code></p> <p>Output class for the generate credential endpoint.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>ModelMetadata</code> <p>The model used to generate the credential</p> <code>credential</code> <code>str</code> <p>The generated credential</p> <code>authenticity</code> <code>float</code> <p>The authenticity of the audio sample used</p> <code>input_audio_duration</code> <code>float</code> <p>The duration of the input audio</p> <code>net_speech_duration</code> <code>float</code> <p>The duration of the speech in the audio</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class GenerateCredentialOutput(DaspeakResponse):\n    \"\"\"Output class for the generate credential endpoint.\n\n    Attributes:\n        model: The model used to generate the credential\n        credential: The generated credential\n        authenticity: The authenticity of the audio sample used\n        input_audio_duration: The duration of the input audio\n        net_speech_duration: The duration of the speech in the audio\n\n    \"\"\"\n\n    model: ModelMetadata\n    credential: str\n    authenticity: float\n    input_audio_duration: float\n    net_speech_duration: float\n\n    @field_validator(\"authenticity\")\n    def round_authenticity(cls, value: float) -&gt; float:\n        return round(value, 3)\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.ModelMetadata","title":"<code>ModelMetadata</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata of the model used to generate the credential.</p> <p>Attributes:</p> Name Type Description <code>hash</code> <code>str</code> <p>The hash of the model</p> <code>mode</code> <code>str</code> <p>The mode of the model</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class ModelMetadata(BaseModel):\n    \"\"\"Metadata of the model used to generate the credential.\n\n    Attributes:\n        hash: The hash of the model\n        mode: The mode of the model\n\n    \"\"\"\n\n    hash: str\n    mode: str\n</code></pre>"},{"location":"api_docs/daspeak/models/#vericlient.daspeak.models.ModelsOutput","title":"<code>ModelsOutput</code>","text":"<p>               Bases: <code>DaspeakResponse</code></p> <p>Output class for the get models endpoint.</p> <p>Attributes:</p> Name Type Description <code>models</code> <code>list</code> <p>The available models in the service</p> Source code in <code>src/vericlient/daspeak/models.py</code> <pre><code>class ModelsOutput(DaspeakResponse):\n    \"\"\"Output class for the get models endpoint.\n\n    Attributes:\n        models: The available models in the service\n\n    \"\"\"\n\n    models: list\n</code></pre>"}]}